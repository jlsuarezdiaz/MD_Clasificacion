---
title: "knn"
author: "Juan Luis Suárez Díaz"
date: "10 de febrero de 2019"
output: pdf_document
---

```{r}
train <- read.csv("train.csv", na.strings = c("?", "NA", "NR", "na", "NaN", "nan"))
train$C <- as.factor(train$C)
test <- read.csv("test.csv", na.strings = c("?", "NA", "NR", "na", "NaN", "nan"))
sample <- read.csv("sampleSubmission.csv")

## BIBLIOTECAS

library(ggplot2)
library(caret)
library(RKEEL)
# library(rDML) # Por si acaso
library(kknn)
library(GGally)
library(Hmisc)
library(dplyr)
library(corrplot)
library(tidyr)
library(VIM)
library(mice)
library(bmrm)
library(DMwR)
library(NoiseFiltersR)
```

```{r}
## ANÁLISIS

# Resumen de los datos
summary(train)
summary(test)
```

```{r}
describe(train)
describe(test)
```

```{r}
# Primera cosa rara: hay datos basura con -68000 y pico en todas sus variables.

train[apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)),]

# Son todo números casi iguales y tienen clases distintas. Quizás es mejor eliminarlos todos. Problema: también hay datos de este tipo en test
```

```{r}
test[apply(test, MARGIN=1, function(x) any(!is.na(x) & x < -68000)),]
# A estas mierdas hay que asignarles la clase de alguna forma.
```

```{r}
ggplot(train[apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)),], aes(x=C, fill=C)) + geom_histogram(stat="count")
# En train hay bastantes más ceros, tal vez lo mejor sea asignar todas las test a 0.
```

```{r}
# Índices de estos outliers por si hay que quitarlos
outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) && x < -68000)))
```

```{r}
# Valores perdidos
has.na <- function(x) apply(x,1,function(z)any(is.na(z)))
train[has.na(train),] # 1366 ejemplos con na en train
test[has.na(test),] # No hay NAs en test
# Me suena que dijo salva que test estaba limpio, no solo de nas, sino de ruido también
```

```{r}
# NAs por variable
apply(train, 2, function(x) sum(is.na(x)))
```

```{r}
# NAs por ejemplo
hist(apply(train, 1, function(x) sum(is.na(x))))
# LOL? solo hay un NA por ejemplo oks
# Aunque esto es genial para imputar NAs usando clasificadores (regresores más bien).
```

```{r}
indices.nas.train <- which(has.na(train))
```


```{r}
# Distribución de clases
ggplot(train, aes(x=C, fill=C)) + geom_histogram(stat="count")

train %>%
  group_by(C) %>%
  summarize(n = n())

# En torno a 6000 datos de clase positiva y 3000 de clase negativa :(
```

```{r}
# Correlaciones
corrplot(cor(train[-c(indices.nas.train),-ncol(train)]))
# Muy sospechoso, a ver si quitando los outliers mierda esos
```

```{r}
corrplot(cor(train[-c(indices.nas.train, outliers.train.por.la.cara),-ncol(train)]))
# Esto ya pinta mejor
```

```{r}
# Variables más correladas
cor(train[-c(indices.nas.train, outliers.train.por.la.cara),-ncol(train)]) %>%
  as.data.frame() %>%
  mutate(var1 = rownames(.)) %>%
  gather(var2, value, -var1) %>%
  arrange(desc(value)) %>%
  filter(var1 < var2)
```

```{r}
corrplot(cor(train[-c(indices.nas.train, outliers.train.por.la.cara),c(16,17,33,38,23,15,40,46,31,21,26)]))

```

```{r}
ggplot(train[-outliers.train.por.la.cara,], aes(x=X16, y=X17, col=C)) + geom_point()
# Clarísima la dependencia. Podríamos elegir la que parezca tener menos ruido para clasificar
```

```{r}
ggplot(train[-outliers.train.por.la.cara,], aes(x=X33, y=X38, col=C)) + geom_point()
# No hay correlación lineal pero ahí está pasando claramente algo.
```

```{r}
ggplot(train[-outliers.train.por.la.cara,], aes(x=X16, y=X23, col=C)) + geom_point()
# Ya empieza a verse menos claro
```

```{r}
ggplot(train[-outliers.train.por.la.cara,], aes(x=X15, y=X16, col=C)) + geom_point()
```

```{r}
ggplot(train[-outliers.train.por.la.cara,], aes(x=X17, y=X23, col=C)) + geom_point()
```

```{r}
# Por curiosidad, qué pasará en test?
ggplot(test[-outliers.test.por.la.cara,], aes(x=X16, y=X17)) + geom_point()
ggplot(test[-outliers.test.por.la.cara,], aes(x=X33, y=X38)) + geom_point()
ggplot(test[-outliers.test.por.la.cara,], aes(x=X16, y=X23)) + geom_point()
ggplot(test[-outliers.test.por.la.cara,], aes(x=X15, y=X16)) + geom_point()
ggplot(test[-outliers.test.por.la.cara,], aes(x=X17, y=X23)) + geom_point()
# Bastante parecidos
```

```{r}
# Componentes principales
pca.bruto <- prcomp(train[-c(outliers.train.por.la.cara, indices.nas.train),-ncol(train)], center=T, scale=T)
plot(pca.bruto)
summary(pca.bruto)
# 95 % -> 30 PCs
# 90 % -> 23 PCs
# 85 % -> 18 PCs
# 80 % -> 14 PCs
```

```{r}
# Se verá algo con 2 PCAs?
pca.plano <- cbind(as.data.frame(predict(pca.bruto)[,1:2]), C = train$C[-c(outliers.train.por.la.cara, indices.nas.train)])
ggplot(pca.plano, aes(x=PC1, y=PC2, col=C)) + geom_point()
# Pues no
```

```{r}
# Funcionará aprender distancias?
# library(rDML)
# scaled.train <- as.data.frame(scale(train[,-ncol(train)]))
# dmlmj <- dml$DMLMJ()
# dmlmj$fit(scaled.train[-c(outliers.train.por.la.cara, indices.nas.train),], train[-c(outliers.train.por.la.cara, indices.nas.train),ncol(train)])
# dmlmj$metric()
# dmlmj$transformer()
# Funciona (por lo menos en mi ordenador), aunque demasiado lento, y este es de los rápidos :(
```

```{r}
# Diferencias entre densidades para cada variable (train / test)
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X1)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X1)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X2)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X2)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X3)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X3)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X4)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X4)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X5)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X5)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X6)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X6)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X7)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X7)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X8)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X8)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X9)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X9)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X10)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X10)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X11)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X11)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X12)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X12)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X13)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X13)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X14)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X14)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X15)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X15)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X16)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X16)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X17)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X17)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X18)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X18)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X19)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X19)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X20)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X20)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X21)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X21)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X22)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X22)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X23)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X23)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X24)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X24)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X25)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X25)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X26)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X26)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X27)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X27)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X28)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X28)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X29)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X29)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X30)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X30)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X31)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X31)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X32)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X32)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X33)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X33)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X34)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X34)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X35)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X35)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X36)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X36)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X37)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X37)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X38)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X38)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X39)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X39)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X40)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X40)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X41)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X41)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X42)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X42)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X43)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X43)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X44)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X44)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X45)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X45)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X46)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X46)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X47)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X47)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X48)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X48)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X49)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X49)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X50)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X50)) + geom_density()
  

```
```{r}
# X6 toma negativos en train pero no en test -> o negativos pasan a ser 0 o se eliminan
# Posible outlier en X7 (X7 > 300)
# X45
```


```{r}
createSubmission <- function(pred, filename){
  sub <- cbind(Id = 1:length(pred), Prediction = as.numeric(as.character(pred)))
  write.csv(sub, paste0("subs-knn/",filename), row.names = F)
  sub
}
```

```{r}
library(bmrm)

# Calcula el score de validación cruzada para el dataset
# funcion.train.predict: función(train, test) que entrena el clasificador con train y devuelve las predicciones sobre test
cross_validation <- function(dataset, funcion.train.predict, folds = 10){
  fold.indexes <- balanced.cv.fold(dataset$C)
  return(mean(sapply(1:folds, cross_validation_fold, fold.indexes, dataset, funcion.train.predict)))
}

cross_validation_fold <- function(fold, indexes, dataset, funcion.train.predict){
  train.inds <- which(indexes==fold)
  train <- dataset[train.inds,]
  test <- na.omit(dataset[-train.inds,])
  ypred <- funcion.train.predict(train, test[,-ncol(test)])
  mean(ypred==test$C)
}
```

```{r}
# Probando validación cruzada
set.seed(28)
funcion.train.predict <- function(train, test){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) && x < -68000)))
  indices.nas.train <- which(has.na(train))
  knn.model <- train(C ~ ., train[-c(outliers.train.por.la.cara, indices.nas.train),], method="knn", preProcess = c("center", "scale"))
  
  # Predict
  preds <- predict(knn.model, test)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

cross_validation(train, funcion.train.predict)
```


```{r}
set.seed(28)
train.predict.nas.por.media <- function(train, test){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) && x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  # Imputación de NAs por defecto Mice
  imputados <- mice::mice(train, m=1, method="mean")
  train.completed <- mice::complete(imputados)
  knn.model <- train(C ~ ., train.completed, method="knn", preProcess = c("center", "scale"))
  
  # Predict
  preds <- predict(knn.model, test)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

cross_validation(train, train.predict.nas.por.media)
```

```{r}
# Imputación con knn
set.seed(28)
train.predict.knn.imputation <- function(train, test){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) && x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  train.completed <- knnImputation(train) 
  knn.model <- train(C ~ ., train.completed, method="knn", preProcess = c("center", "scale"), tuneGrid = expand.grid(k=c(1,3,5,7,9,11,13,15)))
  print(knn.model)
  # Predict
  preds <- predict(knn.model, test)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

cross_validation(train, train.predict.knn.imputation)
```

```{r}
# Imputación con knn por clases
set.seed(28)
train.predict.knn.imputation.by.class <- function(train, test){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) && x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  train.completed.0 <- knnImputation(train[train$C == 0,]) 
  train.completed.1 <- knnImputation(train[train$C == 1,])
  train.completed <- rbind(train.completed.0, train.completed.1)
  knn.model <- train(C ~ ., train.completed, method="knn", preProcess = c("center", "scale"), tuneGrid = expand.grid(k=c(1,3,5,7,9,11,13,15)))
  print(knn.model)
  # Predict
  preds <- predict(knn.model, test)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

cross_validation(train, train.predict.knn.imputation.by.class)
```

```{r}
# PCAs
set.seed(28)
train.predict.pca <- function(train, test, ncomps){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) && x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  train.completed <- knnImputation(train)
  pca <- prcomp(train.completed[,-ncol(train.completed)], center=T, scale=T)
  train.pca <- as.data.frame(predict(pca, train.completed)[,1:ncomps])
  knn.model <- train(C ~ ., cbind(train.pca,C = train.completed$C), method="knn", tuneGrid = expand.grid(k=c(15)))
  print(knn.model)
  # Predict
  test.pca <- as.data.frame(predict(pca, test)[, 1:ncomps])
  preds <- predict(knn.model, test.pca)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

# cross_validation(train, function(train, test) train.predict.pca(train, test, 50))
pca.scores <- sapply(2:50, function(i) cross_validation(train, function(tr, ts) train.predict.pca(tr, ts, i)))
```

```{r}
# Imputación con knn por clases, probando k=1 (Salva dice que debe ir mejor, aunque la CV dice lo contrario)
set.seed(28)
train.predict.knn.imputation.k.1 <- function(train, test){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) && x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  train.completed <- knnImputation(train) 
  knn.model <- train(C ~ ., train.completed, method="knn", preProcess = c("center", "scale"), tuneGrid = expand.grid(k=c(1)),
                     trControl = trainControl(method="none"))
  print(knn.model)
  # Predict
  preds <- predict(knn.model, test)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

cross_validation(train, train.predict.knn.imputation.k.1)
```

```{r}
set.seed(28)
train.predict.knn.imputation.mas.precisa <- function(train, test){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) && x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  train.completed <- data.frame(train)
  nas.ordenados <- order(apply(train, 2, function(x) sum(is.na(x))), decreasing = T)[-51]
  
  # Imputación knn encadenada
  for(i in 1:50){
    indices.nas.train <- which(has.na(train.completed))
    print(length(indices.nas.train))
    na.labels.train <- train.completed[-indices.nas.train,nas.ordenados[i]]
    na.data.train <- train.completed[-indices.nas.train, -c(nas.ordenados[i], 50)]
    na.data.test <- train.completed[which(is.na(train[,nas.ordenados[i]])), -c(nas.ordenados[i], 50)]
    knn.na.model <- train(na.data.train, na.labels.train, 
                          method="knn", preProcess=c("center", "scale"), tuneGrid = expand.grid(k=c(1,3,5,7,9,11,13,15)))
    na.labels.test <- predict(knn.na.model, na.data.test)
    train.completed[which(is.na(train[,nas.ordenados[i]])), nas.ordenados[i]] <- na.labels.test
  }
  knn.model <- train(C ~ ., train.completed, method="knn", preProcess = c("center", "scale"), tuneGrid = expand.grid(k=c(1,3,5,7,9,11,13,15)))
  print(knn.model)
  
  # Predict
  preds <- predict(knn.model, test)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

  cross_validation(train, train.predict.knn.imputation.mas.precisa)
```

```{r}
# Imputación con knn + EF por consenso
set.seed(28)
train.predict.knn.imputation.ef <- function(train, test){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) && x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  train.completed <- knnImputation(train) 
  train.cleaned <- EF(train.completed)$cleanData
  knn.model <- train(C ~ ., train.cleaned, method="knn", preProcess = c("center", "scale"), tuneGrid = expand.grid(k=c(1,3,5,7,9,11,13,15)))
  print(knn.model)
  # Predict
  preds <- predict(knn.model, test)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

cross_validation(train, train.predict.knn.imputation.ef)
```

```{r}
# Imputación con knn + EF por mayoria
set.seed(28)
train.predict.knn.imputation.ef.majority <- function(train, test){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) && x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  train.completed <- knnImputation(train) 
  train.cleaned <- EF(train.completed, consensus = F)$cleanData
  knn.model <- train(C ~ ., train.cleaned, method="knn", preProcess = c("center", "scale"), tuneGrid = expand.grid(k=c(1,3,5,7,9,11,13,15)))
  print(knn.model)
  # Predict
  preds <- predict(knn.model, test)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

cross_validation(train, train.predict.knn.imputation.ef.majority)
```

```{r}
# Imputación con knn + CVCF por consenso
set.seed(28)
train.predict.knn.imputation.cvcf <- function(train, test){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) && x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  train.completed <- knnImputation(train) 
  train.cleaned <- CVCF(train.completed, consensus = T)$cleanData
  knn.model <- train(C ~ ., train.cleaned, method="knn", preProcess = c("center", "scale"), tuneGrid = expand.grid(k=c(1,3,5,7,9,11,13,15)))
  print(knn.model)
  # Predict
  preds <- predict(knn.model, test)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

cross_validation(train, train.predict.knn.imputation.cvcf)
```

```{r}
# Imputación con knn + CVCF por mayoría
set.seed(28)
train.predict.knn.imputation.cvcf.majority <- function(train, test){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) && x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  train.completed <- knnImputation(train) 
  train.cleaned <- CVCF(train.completed, consensus = F)$cleanData
  knn.model <- train(C ~ ., train.cleaned, method="knn", preProcess = c("center", "scale"), tuneGrid = expand.grid(k=c(1,3,5,7,9,11,13,15)))
  print(knn.model)
  # Predict
  preds <- predict(knn.model, test)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

cross_validation(train, train.predict.knn.imputation.cvcf.majority)
```

```{r}
# Imputación con knn + IPF por consenso
set.seed(28)
train.predict.knn.imputation.ipf <- function(train, test){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) && x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  train.completed <- knnImputation(train) 
  train.cleaned <- IPF(train.completed, consensus = T)$cleanData
  knn.model <- train(C ~ ., train.cleaned, method="knn", preProcess = c("center", "scale"), tuneGrid = expand.grid(k=c(1,3,5,7,9,11,13,15)))
  print(knn.model)
  # Predict
  preds <- predict(knn.model, test)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

cross_validation(train, train.predict.knn.imputation.ipf)
```

```{r}
# Imputación con knn + IPF por mayoría
set.seed(28)
train.predict.knn.imputation.ipf.majority <- function(train, test){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) && x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  train.completed <- knnImputation(train) 
  train.cleaned <- IPF(train.completed, consensus = F)$cleanData
  knn.model <- train(C ~ ., train.cleaned, method="knn", preProcess = c("center", "scale"), tuneGrid = expand.grid(k=c(1,3,5,7,9,11,13,15)))
  print(knn.model)
  # Predict
  preds <- predict(knn.model, test)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

cross_validation(train, train.predict.knn.imputation.ipf.majority)
```

```{r}
#########################################################################################################################3
```

```{r}
## SUBIDA 1

set.seed(28)
sub.prueba <- funcion.train.predict(train, test)

sub <- createSubmission(sub.prueba, "prueba") # 0.90045 ??????????
```


```{r}
## SUBIDA 2
# Igual que antes, pero remplazando NAs por la media de la columna.

set.seed(28)
sub.prueba2 <- train.predict.nas.por.media(train, test)

createSubmission(sub.prueba2, "prueba2")
```

```{r}
## SUBIDA 3
# Imputación de NAs con knn
set.seed(28)
sub.3 <- train.predict.knn.imputation(train, test)
createSubmission(sub.3, "3")
```

```{r}
## SUBIDA 4
# Imputación de NAs con knn por clases
set.seed(28)
sub.4 <- train.predict.knn.imputation.by.class(train, test)
createSubmission(sub.4, "4")
```

```{r}
## SUBIDA 5
# Imputación knn + PCA al 95 %
set.seed(28)
sub.5 <- train.predict.pca(train, test, 30)
createSubmission(sub.5, "5")
```

```{r}
## SUBIDA 6
# Imputación knn + 1-NN (prueba)
set.seed(28)
sub.6 <- train.predict.knn.imputation.k.1(train, test)
createSubmission(sub.6, "6") # 0.79 osea que no k=1
```

```{r}
## SUBIDA 7
# Imputación knn encadenada
set.seed(28)
sub.7 <- train.predict.knn.imputation.mas.precisa(train, test)
createSubmission(sub.7, "7")
```

```{r}
## SUBIDA 8
# Imputación knn + EF por consenso
set.seed(28)
sub.8 <- train.predict.knn.imputation.ef(train, test)
createSubmission(sub.8, "8")
```

```{r}
## SUBIDA 9
# Imputación knn + EF por mayoría
set.seed(28)
sub.9 <- train.predict.knn.imputation.ef.majority(train, test)
createSubmission(sub.9, "9")
```

```{r}
## SUBIDA 10
# Imputación knn + CVCF por consenso
set.seed(28)
sub.10 <- train.predict.knn.imputation.cvcf(train, test)
createSubmission(sub.10, "10")
```

```{r}
## SUBIDA 11
# Imputación knn + CVCF por mayoría
set.seed(28)
sub.11 <- train.predict.knn.imputation.cvcf.majority(train, test)
createSubmission(sub.11, "11")
```

```{r}
## SUBIDA 12
# Imputación knn + IPF por consenso
set.seed(28)
sub.12 <- train.predict.knn.imputation.ipf(train, test)
createSubmission(sub.12, "12")
```

```{r}
## SUBIDA 13
# Imputación knn + IPF por mayoría
set.seed(28)
sub.13 <- train.predict.knn.imputation.ipf.majority(train, test)
createSubmission(sub.13, "13")
```
