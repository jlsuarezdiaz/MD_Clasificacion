---
title: "knn2"
author: "Juan Luis Suárez Díaz"
date: "24 de febrero de 2019"
output: pdf_document
---

```{r}
train <- read.csv("train.csv", na.strings = c("?", "NA", "NR", "na", "NaN", "nan"))
train$C <- as.factor(train$C)
test <- read.csv("test.csv", na.strings = c("?", "NA", "NR", "na", "NaN", "nan"))
sample <- read.csv("sampleSubmission.csv")

## BIBLIOTECAS

library(ggplot2)
library(caret)
library(RKEEL)
# library(rDML) # Por si acaso
library(kknn)
library(GGally)
library(Hmisc)
library(dplyr)
library(corrplot)
library(tidyr)
library(VIM)
library(mice)
library(bmrm)
library(DMwR)
library(NoiseFiltersR)
library(beeswarm)
library(moments)
library(MASS)
library(FSelector)
library(Gmedian)
```

```{r}
## ANÁLISIS

# Resumen de los datos
summary(train)
summary(test)
```

```{r}
describe(train)
describe(test)
```

```{r}
# Primera cosa rara: hay datos basura con -68000 y pico en todas sus variables.

train[apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)),]

# Son todo números casi iguales y tienen clases distintas. Quizás es mejor eliminarlos todos. Problema: también hay datos de este tipo en test
```

```{r}
test[apply(test, MARGIN=1, function(x) any(!is.na(x) & x < -68000)),]
# A estas mierdas hay que asignarles la clase de alguna forma.
```

```{r}
ggplot(train[apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)),], aes(x=C, fill=C)) + geom_histogram(stat="count")
# En train hay bastantes más ceros, tal vez lo mejor sea asignar todas las test a 0.
```

```{r}
# Índices de estos outliers por si hay que quitarlos
outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
```

```{r}
# Valores perdidos
has.na <- function(x) apply(x,1,function(z)any(is.na(z)))
train[has.na(train),] # 1366 ejemplos con na en train
test[has.na(test),] # No hay NAs en test
# Me suena que dijo salva que test estaba limpio, no solo de nas, sino de ruido también
```

```{r}
# NAs por variable
apply(train, 2, function(x) sum(is.na(x)))
```

```{r}
# NAs por ejemplo
hist(apply(train, 1, function(x) sum(is.na(x))))
# LOL? solo hay un NA por ejemplo oks
# Aunque esto es genial para imputar NAs usando clasificadores (regresores más bien).
```

```{r}
indices.nas.train <- which(has.na(train))
```


```{r}
# Distribución de clases
ggplot(train, aes(x=C, fill=C)) + geom_histogram(stat="count")

train %>%
  group_by(C) %>%
  summarize(n = n())

# En torno a 6000 datos de clase positiva y 3000 de clase negativa :(
```

```{r, cache=T}
# Correlaciones
corrplot(cor(train[-c(indices.nas.train),-ncol(train)]))
# Muy sospechoso, a ver si quitando los outliers mierda esos
```

```{r}
corrplot(cor(train[-c(indices.nas.train, outliers.train.por.la.cara),-ncol(train)]))
# Esto ya pinta mejor
```

```{r}
# Variables más correladas
cor(train[-c(indices.nas.train, outliers.train.por.la.cara),-ncol(train)]) %>%
  as.data.frame() %>%
  mutate(var1 = rownames(.)) %>%
  gather(var2, value, -var1) %>%
  arrange(desc(abs(value))) %>%
  filter(var1 < var2) %>%
  head(n=10)
```

```{r}
corrplot(cor(train[-c(indices.nas.train, outliers.train.por.la.cara),c(16,17,33,38,23,15,40,46,26,44,28,37)]))

```

```{r}
ggplot(train[-outliers.train.por.la.cara,], aes(x=X16, y=X17, col=C)) + geom_point()
# Clarísima la dependencia. Podríamos elegir la que parezca tener menos ruido para clasificar
```

```{r}
ggplot(train[-outliers.train.por.la.cara,], aes(x=X33, y=X38, col=C)) + geom_point()
# No hay correlación lineal pero ahí está pasando claramente algo.
```

```{r}
ggplot(train[-outliers.train.por.la.cara,], aes(x=X16, y=X23, col=C)) + geom_point()
# Ya empieza a verse menos claro
```

```{r}
ggplot(train[-outliers.train.por.la.cara,], aes(x=X15, y=X16, col=C)) + geom_point()
```

```{r}
ggplot(train[-outliers.train.por.la.cara,], aes(x=X40, y=X46, col=C)) + geom_point()
```

```{r}
# Por curiosidad, qué pasará en test?
ggplot(test[-outliers.test.por.la.cara,], aes(x=X16, y=X17)) + geom_point()
ggplot(test[-outliers.test.por.la.cara,], aes(x=X33, y=X38)) + geom_point()
ggplot(test[-outliers.test.por.la.cara,], aes(x=X16, y=X23)) + geom_point()
ggplot(test[-outliers.test.por.la.cara,], aes(x=X15, y=X16)) + geom_point()
ggplot(test[-outliers.test.por.la.cara,], aes(x=X17, y=X23)) + geom_point()
# Bastante parecidos
```

```{r}
# Componentes principales
pca.bruto <- prcomp(train[-c(outliers.train.por.la.cara, indices.nas.train),-ncol(train)], center=T, scale=T)
plot(pca.bruto)
summary(pca.bruto)
# 95 % -> 30 PCs
# 90 % -> 23 PCs
# 85 % -> 18 PCs
# 80 % -> 14 PCs
```

```{r}
# Se verá algo con 2 PCAs?
pca.plano <- cbind(as.data.frame(predict(pca.bruto)[,1:2]), C = train$C[-c(outliers.train.por.la.cara, indices.nas.train)])
ggplot(pca.plano, aes(x=PC1, y=PC2, col=C)) + geom_point()
# Pues no
```

```{r}
# Funcionará aprender distancias?
# library(rDML)
# scaled.train <- as.data.frame(scale(train[,-ncol(train)]))
# dmlmj <- dml$DMLMJ()
# dmlmj$fit(scaled.train[-c(outliers.train.por.la.cara, indices.nas.train),], train[-c(outliers.train.por.la.cara, indices.nas.train),ncol(train)])
# dmlmj$metric()
# dmlmj$transformer()
# Funciona (por lo menos en mi ordenador), aunque demasiado lento, y este es de los rápidos :(
```

```{r}
# Diferencias entre densidades para cada variable (train / test)
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X1)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X1)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X2)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X2)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X3)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X3)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X4)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X4)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X5)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X5)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X6)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X6)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X7)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X7)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X8)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X8)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X9)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X9)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X10)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X10)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X11)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X11)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X12)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X12)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X13)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X13)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X14)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X14)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X15)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X15)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X16)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X16)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X17)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X17)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X18)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X18)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X19)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X19)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X20)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X20)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X21)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X21)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X22)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X22)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X23)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X23)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X24)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X24)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X25)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X25)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X26)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X26)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X27)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X27)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X28)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X28)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X29)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X29)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X30)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X30)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X31)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X31)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X32)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X32)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X33)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X33)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X34)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X34)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X35)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X35)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X36)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X36)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X37)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X37)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X38)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X38)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X39)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X39)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X40)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X40)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X41)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X41)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X42)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X42)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X43)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X43)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X44)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X44)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X45)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X45)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X46)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X46)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X47)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X47)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X48)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X48)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X49)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X49)) + geom_density()
  
  ggplot(train[-outliers.train.por.la.cara,], aes(x=X50)) + geom_density()
  ggplot(test[-outliers.test.por.la.cara,], aes(x=X50)) + geom_density()
  

```


```{r}
# X6 toma negativos en train pero no en test -> o negativos pasan a ser 0 o se eliminan
# Posible outlier en X7 (X7 > 300)
# X45
```

```{r}
# No había caído antes, pero aunque los outliers no afectan directamente al knn (los vecinos cercanos no van a ser outliers),
# si afecta mucho a la normalización, por lo que hay que quitarlos. Para el z-score deberían afectar solo los univariantes
vector_claves_outliers_IQR_en_alguna_columna <-  function(datos, coef = 1.5){
  return(unique(unlist(sapply(1:ncol(datos),vector_claves_outliers_IQR, 
                       datos=datos, coef=coef))))
}

length(vector_claves_outliers_IQR_en_alguna_columna(train[-c(outliers.train.por.la.cara, indices.nas.train),-ncol(train)], coef=1.5))
length(vector_claves_outliers_IQR_en_alguna_columna(train[-c(outliers.train.por.la.cara, indices.nas.train),-ncol(train)], coef=3.0))
# 4000 normales y 2000 extremos, parecen demasiados
```

```{r}
length(vector_claves_outliers_IQR_en_alguna_columna(test[-c(outliers.test.por.la.cara),], coef=1.5))
length(vector_claves_outliers_IQR_en_alguna_columna(test[-c(outliers.test.por.la.cara),], coef=3.0))

```

```{r}
for(i in 1:50){
  boxplot(cbind(train[-c(outliers.train.por.la.cara),i], test[-c(outliers.test.por.la.cara),i]), main=i)
}
```

```{r}
# Posibles outliers univariantes
# X1 > 1000
# X7 > 300
# X15 > 600
# X16 > 400000
# X17 > 250
# X20 > 300
# X21 < -1300
# X24 > 1700
# X26 < -1500
# X29 > 39
# X33 > 480
# X39 > 400
# X43 > 2000
# X45 > 25
# Hacer X45[X45 < 0] = 0

# Posibles outliers refinados
# X1 > 1000
# X3 > 300000
# X7 > 300
# X38 > 400
# X43 > 2000
# 
```

```{r}
# Simetría y curtosis (diferencia entre centro y extremos respecto a la normal.)
skewness(train[-c(outliers.train.por.la.cara, indices.nas.train),-ncol(train)])
kurtosis(train[-c(outliers.train.por.la.cara, indices.nas.train),-ncol(train)])-3

skewness(test[-c(outliers.test.por.la.cara),-ncol(test)])
kurtosis(test[-c(outliers.test.por.la.cara),-ncol(test)])-3
# Hay variables demasiado asimétricas y eso puede estorbar mucho en la normalización por z-score
```










```{r}
#################################################################################################################
#################################################################################################################
#################################################################################################################
#################################################################################################################
```

```{r}
createSubmission <- function(pred, filename){
  sub <- cbind(Id = 1:length(pred), Prediction = as.numeric(as.character(pred)))
  write.csv(sub, paste0("subs-knn/",filename), row.names = F)
  sub
}
```

```{r}
library(bmrm)

# Calcula el score de validación cruzada para el dataset
# funcion.train.predict: función(train, test) que entrena el clasificador con train y devuelve las predicciones sobre test
cross_validation <- function(dataset, funcion.train.predict, folds = 5){
  fold.indexes <- balanced.cv.fold(dataset$C)
  return(mean(sapply(1:folds, cross_validation_fold, fold.indexes, dataset, funcion.train.predict)))
}

cross_validation_fold <- function(fold, indexes, dataset, funcion.train.predict){
  test.inds <- which(indexes==fold)
  train <- dataset[-test.inds,]
  test <- wrong.remove(na.omit(dataset[test.inds,]))
  ypred <- funcion.train.predict(train, test[,-ncol(test)])
  mean(ypred==test$C)
}
```










```{r}
# 11 + tuneado mas fuerte (Lo mejor hasta la fecha)
set.seed(28)
train.predict.15 <- function(train, test, k, kernel){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  train.completed <- knnImputation(train) 
  train.cleaned <- CVCF(train.completed, consensus = F)$cleanData
  scaler <- preProcess(train.cleaned) # Centrado y escalado
  train.scaled <- predict(scaler, train.cleaned)
  knn.model <- train.kknn(C ~ ., train.scaled, ks = k, kernel = kernel, scale=F)
  # Predict
  test.scaled <- predict(scaler, test)
  preds <- predict(knn.model, test.scaled)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

train.predict.15.with.cleaned.data <- function(train, test, k, kernel){
  scaler <- preProcess(train) # Centrado y escalado
  train.scaled <- predict(scaler, train)
  knn.model <- train.kknn(C ~ ., train.scaled, ks = k, kernel = kernel, scale=F)
  test.scaled <- predict(scaler, test)
  preds <- predict(knn.model, test.scaled)
  return(preds)
}

ks <- c(11,13,15,17,19,21,23)
kernels <- c("rectangular", "triangular", "epanechnikov", "biweight", "triweight", "cos", "inv", "gaussian", "optimal")
cv.all <- data.frame()
cv.cleaned <- data.frame()
for(k in ks){
  for(kernel in kernels){
    tp <- function(tr, tst) train.predict.15(tr, tst, k, kernel)
    print(paste0("K = ", k, "; KERNEL = ", kernel))
    cv.i <- cross_validation(train, tp)
    print(cv.i)
    cv.all[as.character(k), kernel] <- cv.i
  }
}

train.revised <- train[-outliers.train.por.la.cara,]
train.completed <- knnImputation(train.revised)
train.cleaned <- CVCF(train.completed, consensus=F)$cleanData
for(k in ks){
  for(kernel in kernels){
    tp <- function(tr, tst) train.predict.15.with.cleaned.data(tr, tst, k, kernel)
    print(paste0("K = ", k, "; KERNEL = ", kernel))
    cv.i <- cross_validation(train.cleaned, tp)
    print(cv.i)
    cv.cleaned[as.character(k), kernel] <- cv.i
  }
}

```


```{r}
# 16 + posibles outliers
set.seed(28)
train.predict.24 <- function(train, test, k, kernel){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  indices.outliers <- which(
    train$X1 > 1000    |
    train$X7 > 300     |
    train$X15 > 600    |
    train$X16 > 400000 |
    train$X17 > 250    |
    train$X20 > 300    |
    train$X21 < -1300  |
    train$X24 > 1700   |
    train$X26 < -1500  |
    train$X29 > 39     |
    train$X33 > 480    |
    train$X39 > 400    |
    train$X43 > 2000   |
    train$X45 > 25
  )
  if(length(indices.outliers) > 0) train <- train[-indices.outliers,]
  print(paste0("Eliminados ",length(indices.outliers), " outliers."))
  train.completed <- knnImputation(train) 
  train.cleaned <- CVCF(train.completed, consensus = F)$cleanData
  scaler <- preProcess(train.cleaned) # Centrado y escalado
  train.scaled <- predict(scaler, train.cleaned)
  knn.model <- train.kknn(C ~ ., train.scaled, ks = k, kernel = kernel, scale=F)
  # Predict
  test.scaled <- predict(scaler, test)
  preds <- predict(knn.model, test.scaled)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

train.predict.24.with.cleaned.data <- function(train, test, k, kernel){
  scaler <- preProcess(train) # Centrado y escalado
  train.scaled <- predict(scaler, train)
  knn.model <- train.kknn(C ~ ., train.scaled, ks = k, kernel = kernel, scale=F)
  test.scaled <- predict(scaler, test)
  preds <- predict(knn.model, test.scaled)
  return(preds)
}

ks <- c(11,13,15,17,19,21,23)
kernels <- c("rectangular", "triangular", "epanechnikov", "biweight", "triweight", "cos", "inv", "gaussian", "optimal")
cv.all <- data.frame()
cv.cleaned <- data.frame()
for(k in ks){
  for(kernel in kernels){
    tp <- function(tr, tst) train.predict.24(tr, tst, k, kernel)
    print(paste0("K = ", k, "; KERNEL = ", kernel))
    cv.i <- cross_validation(train, tp)
    print(cv.i)
    cv.all[as.character(k), kernel] <- cv.i
  }
}

indices.outliers <- which(
    train$X1 > 1000    |
    train$X7 > 300     |
    train$X15 > 600    |
    train$X16 > 400000 |
    train$X17 > 250    |
    train$X20 > 300    |
    train$X21 < -1300  |
    train$X24 > 1700   |
    train$X26 < -1500  |
    train$X29 > 39     |
    train$X33 > 480    |
    train$X39 > 400    |
    train$X43 > 2000   |
    train$X45 > 25
  )
print(paste0("Eliminados ",length(indices.outliers), " outliers"))
train.revised <- train[-c(outliers.train.por.la.cara, indices.outliers),]
train.completed <- knnImputation(train.revised)
train.cleaned <- CVCF(train.completed, consensus=F)$cleanData
for(k in ks){
  for(kernel in kernels){
    tp <- function(tr, tst) train.predict.24.with.cleaned.data(tr, tst, k, kernel)
    print(paste0("K = ", k, "; KERNEL = ", kernel))
    cv.i <- cross_validation(train.cleaned, tp)
    print(cv.i)
    cv.cleaned[as.character(k), kernel] <- cv.i
  }
}

```


```{r}
# 16 + posibles outliers
set.seed(28)
train.predict.26 <- function(train, test, k, kernel){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  
  train.completed <- knnImputation(train) 
  train.cleaned <- CVCF(train.completed, consensus = F)$cleanData
  indices.outliers <- which(
    train.cleaned$X1 > 1000   |
    train.cleaned$X3 > 300000 |
    train.cleaned$X7 > 300    |
    train.cleaned$X38 > 400   |
    train.cleaned$X43 > 2000  
  )
  if(length(indices.outliers) > 0) train.cleaned <- train.cleaned[-indices.outliers,]
  print(paste0("Eliminados ",length(indices.outliers), " outliers."))
  scaler <- preProcess(train.cleaned) # Centrado y escalado
  train.scaled <- predict(scaler, train.cleaned)
  knn.model <- train.kknn(C ~ ., train.scaled, ks = k, kernel = kernel, scale=F)
  # Predict
  test.scaled <- predict(scaler, test)
  preds <- predict(knn.model, test.scaled)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

train.predict.26.with.cleaned.data <- function(train, test, k, kernel){
  scaler <- preProcess(train) # Centrado y escalado
  train.scaled <- predict(scaler, train)
  knn.model <- train.kknn(C ~ ., train.scaled, ks = k, kernel = kernel, scale=F)
  test.scaled <- predict(scaler, test)
  preds <- predict(knn.model, test.scaled)
  return(preds)
}

ks <- c(15,17,19,21,23,25,27)
kernels <- c("rectangular", "triangular", "epanechnikov", "cos", "inv", "gaussian", "optimal")
cv.all <- data.frame()
cv.cleaned <- data.frame()
for(k in ks){
  for(kernel in kernels){
    tp <- function(tr, tst) train.predict.26(tr, tst, k, kernel)
    print(paste0("K = ", k, "; KERNEL = ", kernel))
    cv.i <- cross_validation(train, tp)
    print(cv.i)
    cv.all[as.character(k), kernel] <- cv.i
  }
}


train.revised <- train[-c(outliers.train.por.la.cara),]
train.completed <- knnImputation(train.revised)
train.cleaned <- CVCF(train.completed, consensus=F)$cleanData
indices.outliers <- which(
    train.cleaned$X1 > 1000   |
    train.cleaned$X3 > 300000 |
    train.cleaned$X7 > 300    |
    train.cleaned$X38 > 400   |
    train.cleaned$X43 > 2000  
)
train.cleaned <- train.cleaned[-indices.outliers,]
print(paste0("Eliminados ",length(indices.outliers), " outliers"))
for(k in ks){
  for(kernel in kernels){
    tp <- function(tr, tst) train.predict.26.with.cleaned.data(tr, tst, k, kernel)
    print(paste0("K = ", k, "; KERNEL = ", kernel))
    cv.i <- cross_validation(train.cleaned, tp)
    print(cv.i)
    cv.cleaned[as.character(k), kernel] <- cv.i
  }
}
```

```{r}
nm <- names(train.cleaned)
i <- 5
ggplot(train.cleaned, aes_string(y=nm[i], fill="C")) + geom_boxplot()

```
```{r}
ggplot(train.cleaned, aes(y=X5*X5, fill=C)) + geom_boxplot()
```


```{r}

cbrt <- function(x) sign(x) * abs(x)^(1/3)
# Estudio de outliers, simetría y transformaciones a partir de los boxplots, uno a uno.
# X1 bien
# X2 le siente bien un logaritmo. posible outlier en X2 > 6 y clase 1
# X3 también le sienta bien un logaritmo.
# X4 bien, pero también le sienta bien logaritmo
# X5 bien
# X6 bien, X6 < 0 poner a 0
# X7 le sienta bien logaritmo si eliminamos los X7 = 0
# X8 tal vez X^2?
# X9 le sienta bien logaritmo, posible outlier en X9 > 400 y clase 1
# X10 bien
# X11 bien
# X12 bien
# X13 le sienta bien al cuadrado
# X14 bien
# X15 le sienta bien logaritmo
# X16 le sienta bien logaritmo
# X17 le sienta bien logaritmo
# X18 le sienta bien logaritmo
# X19 bien
# X20 bien
# X21 le sienta bien raiz cubica ?
# X22 bien
# X23 le sienta bien logaritmo + 100 (< 0 = 0)
# X24 le sienta bien logaritmo
# X25 le sienta bien cuadrado
# X26 le sienta bien raiz cubica
# X27 le sienta bien raíz cúbica
# X28 le sienta bien logaritmo
# X29 le sienta bien logaritmo
# X30 bien
# X31 le sienta bien logaritmo
# X32 bien
# X33 le sienta bien logaritmo
# X34 le sienta bien cuadrado
# X35 le sienta bien log(.+0.1), los no ceros = 0
# X36 bien
# X37 bien
# X38 bien
# X39 le sienta bien log
# X40 le sienta bien cbrt
# X41 bien
# X42 bien
# X43 le sienta bien cbrt
# X44 le sienta bien log
# X45 tal vez sqrt? y <0 a 0
# X46 bien
# X47 le sienta bien cbrt
# X48 log + 25
# X49 cbrt
# X50 bien

wrong.remove <- function(data){
  transf <- data.frame(data)
  wrongs <- which(
    transf$X7 <= 0
  )
  if(length(wrongs) > 0){
    transf <- transf[-wrongs,]
  }
  transf
}

attr.transform <- function(data){
  trans <- wrong.remove(data)
  trans$X2 <- log(trans$X2)
  trans$X3 <- log(trans$X3)
  trans$X4 <- log(trans$X4)
  trans$X6[trans$X6 < 0] <- 0
  trans$X7 <- log(trans$X7)
  trans$X8 <- trans$X8^2
  trans$X9 <- log(trans$X9)
  trans$X13 <- trans$X13^2
  trans$X15 <- log(trans$X15)
  trans$X16 <- log(trans$X16)
  trans$X17 <- log(trans$X17)
  trans$X18 <- log(trans$X18)
  # trans$X21 <- cbrt(trans$X21)
  train$X23[train$X23 < 0] <- 0
  trans$X23 <- log(trans$X23 + 100)
  trans$X24 <- log(trans$X24)
  trans$X25 <- trans$X25^2
  trans$X26 <- cbrt(trans$X26)
  trans$X27 <- cbrt(trans$X27)
  trans$X28 <- log(trans$X28)
  trans$X29 <- log(trans$X29)
  trans$X31 <- log(trans$X31)
  trans$X33 <- log(trans$X33)
  trans$X34 <- trans$X34^2
  trans$X35[trans$X35 < 0] <- 0
  trans$X35 <- log(trans$X35 + 0.1)
  trans$X39 <- log(trans$X39)
  trans$X40 <- cbrt(trans$X40)
  trans$X43 <- cbrt(trans$X43)
  trans$X44 <- log(trans$X44)
  trans$X45[trans$X45 < 0] <- 0 
  trans$X45 <- sqrt(trans$X45)
  trans$X47 <- cbrt(trans$X47)
  trans$X48 <- log(trans$X48 + 25)
  trans$X49 <- cbrt(trans$X49)
  trans
}

attr.transform.add <- function(data){
  trans <- wrong.remove(data)
  
  trans$tX2 <- log(trans$X2)
  trans$tX3 <- log(trans$X3)
  trans$tX4 <- log(trans$X4)
  trans$X6[trans$X6 < 0] <- 0
  trans$tX7 <- log(trans$X7)
  trans$tX8 <- trans$X8^2
  trans$tX9 <- log(trans$X9)
  trans$tX13 <- trans$X13^2
  trans$tX15 <- log(trans$X15)
  trans$tX16 <- log(trans$X16)
  trans$tX17 <- log(trans$X17)
  trans$tX18 <- log(trans$X18)
  trans$X21 <- cbrt(trans$X21)
  train$X23[train$X23 < 0] <- 0
  trans$tX23 <- log(trans$X23 + 100)
  trans$tX24 <- log(trans$X24)
  trans$tX25 <- trans$X25^2
  trans$tX26 <- cbrt(trans$X26)
  trans$tX27 <- cbrt(trans$X27)
  trans$tX28 <- log(trans$X28)
  trans$tX29 <- log(trans$X29)
  trans$tX31 <- log(trans$X31)
  trans$tX33 <- log(trans$X33)
  # trans$tX34 <- trans$X34^2
  trans$X35[trans$X35 < 0] <- 0
  trans$tX35 <- log(trans$X35 + 0.1)
  # trans$tX39 <- log(trans$X39)
  trans$tX40 <- cbrt(trans$X40)
  # trans$tX43 <- cbrt(trans$X43)
  trans$tX44 <- log(trans$X44)
  trans$X45[trans$X45 < 0] <- 0 
  trans$tX45 <- sqrt(trans$X45)
  trans$tX47 <- cbrt(trans$X47)
  trans$tX48 <- log(trans$X48 + 25)
  trans$tX49 <- cbrt(trans$X49)
  
  ##
  trans <- trans[,-c(11,34,39,41,43)]
  # trans <- trans[,-c(34)]
  # trans <- trans[,-c(11,34,39,41,43)]
  # trans <- trans[, -c(11,34,39,41,43,12,13,19,50)]
  ##
  
  trans
}
```


```{r}
# 16 + transformaciones
set.seed(28)
train.predict.27 <- function(train, test, k, kernel){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  train.completed <- knnImputation(train) 
  train.cleaned <- CVCF(train.completed, consensus = F)$cleanData
  train.transformed <- attr.transform(train.cleaned)
  scaler <- preProcess(train.transformed) # Centrado y escalado
  train.scaled <- predict(scaler, train.transformed)
  knn.model <- train.kknn(C ~ ., train.scaled, ks = k, kernel = kernel, scale=F)
  # Predict
  test[outliers.test.por.la.cara,] <- 1
  test.transformed <- attr.transform(test)
  test.scaled <- predict(scaler, test.transformed)
  preds <- predict(knn.model, test.scaled)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

train.predict.27.with.cleaned.data <- function(train, test, k, kernel){
  scaler <- preProcess(train) # Centrado y escalado
  train.scaled <- predict(scaler, train)
  knn.model <- train.kknn(C ~ ., train.scaled, ks = k, kernel = kernel, scale=F)
  test.scaled <- predict(scaler, test)
  preds <- predict(knn.model, test.scaled)
  return(preds)
}

ks <- c(15,17,19,21,23,25)
kernels <- c("rectangular", "triangular", "epanechnikov", "cos", "inv", "gaussian", "optimal")
cv.all <- data.frame()
cv.cleaned <- data.frame()
for(k in ks){
  for(kernel in kernels){
    tp <- function(tr, tst) train.predict.27(tr, tst, k, kernel)
    print(paste0("K = ", k, "; KERNEL = ", kernel))
    cv.i <- cross_validation(train, tp)
    print(cv.i)
    cv.all[as.character(k), kernel] <- cv.i
  }
}

train.revised <- train[-outliers.train.por.la.cara,]
train.completed <- knnImputation(train.revised)
train.cleaned <- CVCF(train.completed, consensus=F)$cleanData
train.transformed <- attr.transform(train.cleaned)
for(k in ks){
  for(kernel in kernels){
    tp <- function(tr, tst) train.predict.27.with.cleaned.data(tr, tst, k, kernel)
    print(paste0("K = ", k, "; KERNEL = ", kernel))
    cv.i <- cross_validation(train.transformed, tp)
    print(cv.i)
    cv.cleaned[as.character(k), kernel] <- cv.i
  }
}

```


```{r}
# 16 + transformaciones añadidas
set.seed(28)
train.predict.29 <- function(train, test, k, kernel){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  train.completed <- knnImputation(train) 
  train.cleaned <- CVCF(train.completed, consensus = F)$cleanData
  train.transformed <- attr.transform.add(train.cleaned) 
  scaler <- preProcess(train.transformed) # Centrado y escalado
  train.scaled <- predict(scaler, train.transformed)
  knn.model <- train.kknn(C ~ ., train.scaled, ks = k, kernel = kernel, scale=F)
  # Predict
  test[outliers.test.por.la.cara,] <- 1
  test.transformed <- attr.transform.add(test)
  test.scaled <- predict(scaler, test.transformed)
  preds <- predict(knn.model, test.scaled)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

train.predict.29.with.cleaned.data <- function(train, test, k, kernel){
  scaler <- preProcess(train) # Centrado y escalado
  train.scaled <- predict(scaler, train)
  knn.model <- train.kknn(C ~ ., train.scaled, ks = k, kernel = kernel, scale=F)
  test.scaled <- predict(scaler, test)
  preds <- predict(knn.model, test.scaled)
  return(preds)
}

ks <- c(15,17,19,21,23,25)
kernels <- c("rectangular", "triangular", "epanechnikov", "cos", "inv", "gaussian", "optimal")
cv.all <- data.frame()
cv.cleaned <- data.frame()
for(k in ks){
  for(kernel in kernels){
    tp <- function(tr, tst) train.predict.29(tr, tst, k, kernel)
    print(paste0("K = ", k, "; KERNEL = ", kernel))
    cv.i <- cross_validation(train, tp)
    print(cv.i)
    cv.all[as.character(k), kernel] <- cv.i
  }
}

train.revised <- train[-outliers.train.por.la.cara,]
train.completed <- knnImputation(train.revised)
train.cleaned <- CVCF(train.completed, consensus=F)$cleanData
train.transformed <- attr.transform.add(train.cleaned) %>% dplyr::select(-C,C)
for(k in ks){
  for(kernel in kernels){
    tp <- function(tr, tst) train.predict.29.with.cleaned.data(tr, tst, k, kernel)
    print(paste0("K = ", k, "; KERNEL = ", kernel))
    cv.i <- cross_validation(train.transformed, tp)
    print(cv.i)
    cv.cleaned[as.character(k), kernel] <- cv.i
  }
}

```

```{r}
# Arma secreta 1
train.revised <- train[-outliers.train.por.la.cara,]
train.completed <- knnImputation(train.revised)
train.cleaned <- CVCF(train.completed, consensus=F)$cleanData
train.transformed <- attr.transform.add(train.cleaned) %>% dplyr::select(-C,C)
scaler <- preProcess(train.transformed)
train.scaled <- predict(scaler, train.transformed)
dmlmj <- dml$DMLMJ(n_neighbors = 17)
dmlmj$fit(train.scaled[,-ncol(train.scaled)], train.scaled[,ncol(train.scaled)])
train.dmlmj <- data.frame(dmlmj$transform(train.scaled[,-ncol(train.scaled)]), C=train.scaled$C)
```

```{r}
# CV para arma secreta 1
set.seed(28)
train.predict.dmlmj <- function(train, test, k, kernel){
  # Train
  outliers.train.por.la.cara <- which(apply(train[,-ncol(train)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  if(length(outliers.train.por.la.cara) > 0) train <- train[-outliers.train.por.la.cara,]
  train.completed <- knnImputation(train) 
  train.cleaned <- CVCF(train.completed, consensus = F)$cleanData
  train.transformed <- attr.transform.add(train.cleaned) %>% dplyr::select(-C,C)
  scaler <- preProcess(train.transformed) # Centrado y escalado
  train.scaled <- predict(scaler, train.transformed)
  dmlmj <- dml$DMLMJ(n_neighbors = 5)
  dmlmj$fit(train.scaled[,-ncol(train.scaled)], train.scaled[,ncol(train.scaled)])
  train.dmlmj <- data.frame(dmlmj$transform(train.scaled[,-ncol(train.scaled)]), C=train.scaled$C)
  knn.model <- train.kknn(C ~ ., train.dmlmj, ks = k, kernel = kernel, scale=F)
  # Predict
  test[outliers.test.por.la.cara,] <- 1 # Para que no estorben en las transformaciones
  test.transformed <- attr.transform.add(test)
  test.scaled <- predict(scaler, test.transformed)
  test.dmlmj <- data.frame(dmlmj$transform(test.scaled))
  preds <- predict(knn.model, test.dmlmj)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}

train.predict.dmlmj.with.cleaned.data <- function(train, test, k, kernel){
  scaler <- preProcess(train) # Centrado y escalado
  train.scaled <- predict(scaler, train)
  dmlmj <- dml$DMLMJ(n_neighbors = 5)
  dmlmj$fit(train.scaled[,-ncol(train.scaled)], train.scaled[,ncol(train.scaled)])
  train.dmlmj <- data.frame(dmlmj$transform(train.scaled[,-ncol(train.scaled)]), C=train.scaled$C)
  knn.model <- train.kknn(C ~ ., train.dmlmj, ks = k, kernel = kernel, scale=F)
  test.scaled <- predict(scaler, test)
  test.dmlmj <- data.frame(dmlmj$transform(test.scaled))
  preds <- predict(knn.model, test.dmlmj)
  return(preds)
}

ks <- c(19,21,23,25)
kernels <- c("rectangular", "triangular", "epanechnikov", "cos", "inv", "gaussian", "optimal")
cv.all.dmlmj <- data.frame()
cv.cleaned.dmlmj <- data.frame()
for(k in ks){
  for(kernel in kernels){
    tp <- function(tr, tst) train.predict.dmlmj(tr, tst, k, kernel)
    print(paste0("K = ", k, "; KERNEL = ", kernel))
    cv.i <- cross_validation(train, tp)
    print(cv.i)
    cv.all.dmlmj[as.character(k), kernel] <- cv.i
  }
}

train.revised <- train[-outliers.train.por.la.cara,]
train.completed <- knnImputation(train.revised)
train.cleaned <- CVCF(train.completed, consensus=F)$cleanData
train.transformed <- attr.transform.add(train.cleaned) %>% dplyr::select(-C,C)
for(k in ks){
  for(kernel in kernels){
    tp <- function(tr, tst) train.predict.dmlmj.with.cleaned.data(tr, tst, k, kernel)
    print(paste0("K = ", k, "; KERNEL = ", kernel))
    cv.i <- cross_validation(train.transformed, tp)
    print(cv.i)
    cv.cleaned.dmlmj[as.character(k), kernel] <- cv.i
  }
}

```


```{r}
train.cleaned.0 <- train.cleaned[train.cleaned$C == 0, ]
train.cleaned.1 <- train.cleaned[train.cleaned$C == 1, ]
num.centroids.0 <- 4000
num.centroids.1 <- 2000

kmeans.0 <- kGmedian(train.cleaned.0[,-ncol(train.cleaned.0)], num.centroids.0)
kmeans.1 <- kGmedian(train.cleaned.1[,-ncol(train.cleaned.1)], num.centroids.1)

train.smoothed.0 <- data.frame(kmeans.0$centers, C = as.factor(rep(0, num.centroids.0)))
train.smoothed.1 <- data.frame(kmeans.1$centers, C = as.factor(rep(1, num.centroids.1)))

train.smoothed <- rbind(train.smoothed.0, train.smoothed.1)

ggplot(train[-outliers.train.por.la.cara,], aes(x=X16, y=X17, col=C)) + geom_point()
ggplot(train.smoothed, aes(x=X16, y=X17, col=C)) + geom_point()
```




```{r}
#########################################################################################################################3
```



```{r}
## SUBIDA 16 (la mejor por ahora)
# 11 + tuneado de ks y kernels (k = 17, kernel=triangular)
set.seed(28)
sub.16 <- train.predict.15(train, test, 17, "triangular")
createSubmission(sub.16, "16") # 0.91220
```


```{r}
## SUBIDA 24
# 16 + outliers (k = 23, kernel=cos)
set.seed(28)
sub.24 <- train.predict.24(train, test, 23, "cos")
createSubmission(sub.24, "24")

```

```{r}
## SUBIDA 25
# 16 + outliers (k = 23, kernel=gaussian)
set.seed(28)
sub.25 <- train.predict.24(train, test, 23, "gaussian")
# createSubmission(sub.25, "25")

```

```{r}
## SUBIDA 26
# 16 + menos outliers (k=23, kernel=triangular)
set.seed(28)
sub.26 <- train.predict.26(train, test, 23, "triangular")
createSubmission(sub.26, "26")
```


```{r}
## SUBIDA 27
# 16 + transformaciones (k=23, kernel=gaussian)
set.seed(28)
sub.27 <- train.predict.27(train, test, 23, "gaussian")
createSubmission(sub.27, "27")
```


```{r}
## SUBIDA 28
# 16 + transformaciones (k=17, kernel=triangular)
set.seed(28)
sub.28 <- train.predict.27(train, test, 17, "triangular")
createSubmission(sub.28, "28")
```


```{r}
## SUBIDA 29
# 16 +  transformaciones añadidas
set.seed(28)
sub.29 <- train.predict.29(train, test, 25, "optimal")
createSubmission(sub.29, "29")
```

```{r}
## SUBIDA 30
# 16 +  transformaciones añadidas
set.seed(28)
sub.30 <- train.predict.29(train, test, 17, "triangular")
createSubmission(sub.30, "30")
```

```{r}
## SUBIDA 31
# Seleccionando transformaciones (todas menos X11, X34, X39, X41, X43) k=17, ker=triangular
# 
set.seed(28)
sub.31 <- train.predict.29(train, test, 17, "triangular")
createSubmission(sub.31, "31") # 0.91424
```


```{r}
## SUBIDA 32
# Seleccionando transformaciones (todas menos X11, X34, X39, X41, X43) k=25 ker=triangular
# 
set.seed(28)
sub.32 <- train.predict.29(train, test, 25, "triangular")
createSubmission(sub.32, "32")
```

```{r}
## SUBIDA 33
# Seleccionando transformaciones (todas menos X11, X34, X39, X41, X43) k=19 ker=epanechnikov
# 
set.seed(28)
sub.33 <- train.predict.29(train, test, 19, "epanechnikov")
createSubmission(sub.33, "33") # 0.91475
```

```{r}
## SUBIDA 34
# Prueba de arma secreta 1
#
set.seed(28)
sub.34 <- train.predict.dmlmj(train, test, 19, "epanechnikov")
createSubmission(sub.34, "34")
```

```{r}
## SUBIDA 35
# Seleccionando transformaciones (X34) k=19 ker=epanechnikov
set.seed(28)
sub.35 <- train.predict.29(train, test, 19, "epanechnikov")
createSubmission(sub.35, "35")
```

```{r}
## SUBIDA 36
# Seleccionando transformaciones (las de 33 sin cambiar variables a 0) k=19 ker=epanechnikov
set.seed(28)
sub.36 <- train.predict.29(train, test, 19, "epanechnikov")
createSubmission(sub.36, "36")
```

```{r}
## SUBIDA 37
# Seleccionando transformaciones (quitando todas con chi2 < 0.2) k=19 ker=epanechnikov (ERROR)
set.seed(28)
sub.37 <- train.predict.29(train, test, 19, "epanechnikov")
createSubmission(sub.37, "37") # 0.
```

```{r}
## SUBIDA 38
# Seleccionando transformaciones (quitando todas con chi2 < 0.2) k=19 ker=epanechnikov
set.seed(28)
sub.38 <- train.predict.29(train, test, 19, "epanechnikov")
createSubmission(sub.38, "38") # 0.
```

```{r}
## SUBIDA 39
# Y si cambiamos el orden? cvcf -> transformaciones -> imputacion
k <- 19
kernel <- "epanechnikov"
set.seed(28)
train.cleaned.2 <- CVCF(train, consensus = F)$cleanData
train.transformed.2 <- attr.transform.add(train.cleaned.2) %>% dplyr::select(-C,C)
train.completed.2 <- knnImputation(train.transformed.2)
scaler <- preProcess(train.completed.2)
train.scaled.2 <- predict(scaler, train.completed.2)
knn.model <- train.kknn(C ~ ., train.scaled.2, ks = k, kernel = kernel, scale=F)

test[outliers.test.por.la.cara,] <- 1 # Para que no estorben en las transformaciones
test.transformed <- attr.transform.add(test)
test.scaled <- predict(scaler, test.transformed)
preds <- predict(knn.model, test.scaled)
preds[outliers.test.por.la.cara] <- 0
sub.39 <- preds
createSubmission(sub.39, "39")
```

```{r}
## SUBIDA 40
# Lo de antes pero quitando los outliers por la cara
k <- 19
kernel <- "epanechnikov"
set.seed(28)
train.sin.outliers <- train[-outliers.train.por.la.cara,]
train.cleaned.2 <- CVCF(train.sin.outliers, consensus = F)$cleanData
train.transformed.2 <- attr.transform.add(train.cleaned.2) %>% dplyr::select(-C,C)
train.completed.2 <- knnImputation(train.transformed.2)
scaler <- preProcess(train.completed.2)
train.scaled.2 <- predict(scaler, train.completed.2)
knn.model <- train.kknn(C ~ ., train.scaled.2, ks = k, kernel = kernel, scale=F)

test[outliers.test.por.la.cara,] <- 1 # Para que no estorben en las transformaciones
test.transformed <- attr.transform.add(test)
test.scaled <- predict(scaler, test.transformed)
preds <- predict(knn.model, test.scaled)
preds[outliers.test.por.la.cara] <- 0
sub.40 <- preds
createSubmission(sub.40, "40")
```

```{r}
## SUBIDA 41
# Ya que la 39 funcionó inesperadamente bien, a ver que tal con otros parámetros
k <- 17
kernel <- "triangular"
set.seed(28)
train.cleaned.2 <- CVCF(train, consensus = F)$cleanData
train.transformed.2 <- attr.transform.add(train.cleaned.2) %>% dplyr::select(-C,C)
train.completed.2 <- knnImputation(train.transformed.2)
scaler <- preProcess(train.completed.2)
train.scaled.2 <- predict(scaler, train.completed.2)
knn.model <- train.kknn(C ~ ., train.scaled.2, ks = k, kernel = kernel, scale=F)

test[outliers.test.por.la.cara,] <- 1 # Para que no estorben en las transformaciones
test.transformed <- attr.transform.add(test)
test.scaled <- predict(scaler, test.transformed)
preds <- predict(knn.model, test.scaled)
preds[outliers.test.por.la.cara] <- 0
sub.41 <- preds
createSubmission(sub.41, "41")
```

```{r}
## SUBIDA 42
# Ya que la 39 funcionó inesperadamente bien, a ver que tal con otros parámetros
k <- 23
kernel <- "gaussian"
set.seed(28)
train.cleaned.2 <- CVCF(train, consensus = F)$cleanData
train.transformed.2 <- attr.transform.add(train.cleaned.2) %>% dplyr::select(-C,C)
train.completed.2 <- knnImputation(train.transformed.2)
scaler <- preProcess(train.completed.2)
train.scaled.2 <- predict(scaler, train.completed.2)
knn.model <- train.kknn(C ~ ., train.scaled.2, ks = k, kernel = kernel, scale=F)

test[outliers.test.por.la.cara,] <- 1 # Para que no estorben en las transformaciones
test.transformed <- attr.transform.add(test)
test.scaled <- predict(scaler, test.transformed)
preds <- predict(knn.model, test.scaled)
preds[outliers.test.por.la.cara] <- 0
sub.42 <- preds
createSubmission(sub.42, "42")
```

```{r}
## SUBIDA 43
# Ya que la 39 funcionó inesperadamente bien, a ver que tal con otros parámetros
k <- 17
kernel <- "epanechnikov"
set.seed(28)
train.cleaned.2 <- CVCF(train, consensus = F)$cleanData
train.transformed.2 <- attr.transform.add(train.cleaned.2) %>% dplyr::select(-C,C)
train.completed.2 <- knnImputation(train.transformed.2)
scaler <- preProcess(train.completed.2)
train.scaled.2 <- predict(scaler, train.completed.2)
knn.model <- train.kknn(C ~ ., train.scaled.2, ks = k, kernel = kernel, scale=F)

testt <- data.frame(test)
testt[outliers.test.por.la.cara,] <- 1 # Para que no estorben en las transformaciones
test.transformed <- attr.transform.add(testt)
test.scaled <- predict(scaler, test.transformed)
preds <- predict(knn.model, test.scaled)
preds[outliers.test.por.la.cara] <- 0
sub.43 <- preds
createSubmission(sub.43, "43") # 0.91832
```

```{r}
## SUBIDA 44
# Lo anterior + NCA
set.seed(28)
k <- 17
kernel <- "epanechnikov"
train.nca.partition <- function(i){
  train.nca.i <- read.csv(paste0("./train_nca_",i,".csv"), header = F)
  train.nca.i$C <- train.scaled.2$C
  names(train.nca.i) <- names(train.scaled.2)
  test.nca.i <- read.csv(paste0("./test_nca_",i,".csv"), header = F)
  names(test.nca.i) <- names(test.scaled)
  knn.model <- train.kknn(C ~ ., train.nca.i, ks=k, kernel=kernel, scale=F)
  preds <- predict(knn.model, test.nca.i)
  preds[outliers.test.por.la.cara] <- 0
  preds
}

sub.44.all <- sapply(0:8, train.nca.partition)
sub.44 <- apply(sub.44.all, 1, function(x) ifelse(sum(x==1) > 4,1,0))
createSubmission(sub.44, "44") # 0.
```

```{r}
## SUBIDA 45
# Lo anterior + NCA (pequeña corrección de lo anterior)
set.seed(28)
k <- 17
kernel <- "epanechnikov"
train.nca.partition.2 <- function(i){
  train.nca.i <- read.csv(paste0("./train_nca_x_",i,".csv"), header = F)
  train.nca.y <- read.csv(paste0("./train_nca_y_",i,".csv"), header = F)
  train.nca.i$C <- as.factor(train.nca.y$V1)
  names(train.nca.i) <- names(train.scaled.2)
  test.nca.i <- read.csv(paste0("./test_nca_",i,".csv"), header = F)
  names(test.nca.i) <- names(test.scaled)
  knn.model <- train.kknn(C ~ ., train.nca.i, ks=k, kernel=kernel, scale=F)
  preds <- predict(knn.model, test.nca.i)
  preds[outliers.test.por.la.cara] <- 0
  preds
}

sub.45.all <- sapply(0:8, train.nca.partition.2)
sub.45 <- apply(sub.45.all, 1, function(x) ifelse(sum(x==1) > 4,1,0))
createSubmission(sub.45, "45") # 0.
```

```{r}
## SUBIDA 46
# Ya que la 39 funcionó inesperadamente bien, a ver que tal con otros parámetros
k <- 15
kernel <- "epanechnikov"
set.seed(28)
train.cleaned.2 <- CVCF(train, consensus = F)$cleanData
train.transformed.2 <- attr.transform.add(train.cleaned.2) %>% dplyr::select(-C,C)
train.completed.2 <- knnImputation(train.transformed.2)
scaler <- preProcess(train.completed.2)
train.scaled.2 <- predict(scaler, train.completed.2)
knn.model <- train.kknn(C ~ ., train.scaled.2, ks = k, kernel = kernel, scale=F)

testt <- data.frame(test)
testt[outliers.test.por.la.cara,] <- 1 # Para que no estorben en las transformaciones
test.transformed <- attr.transform.add(testt)
test.scaled <- predict(scaler, test.transformed)
preds <- predict(knn.model, test.scaled)
preds[outliers.test.por.la.cara] <- 0
sub.46 <- preds
createSubmission(sub.46, "46") # 0.
```

```{r}
## SUBIDA 47
# Ya que la 39 funcionó inesperadamente bien, a ver que tal con otros parámetros
k <- 21
kernel <- "epanechnikov"
set.seed(28)
train.cleaned.2 <- CVCF(train, consensus = F)$cleanData
train.transformed.2 <- attr.transform.add(train.cleaned.2) %>% dplyr::select(-C,C)
train.completed.2 <- knnImputation(train.transformed.2)
scaler <- preProcess(train.completed.2)
train.scaled.2 <- predict(scaler, train.completed.2)
knn.model <- train.kknn(C ~ ., train.scaled.2, ks = k, kernel = kernel, scale=F)

testt <- data.frame(test)
testt[outliers.test.por.la.cara,] <- 1 # Para que no estorben en las transformaciones
test.transformed <- attr.transform.add(testt)
test.scaled <- predict(scaler, test.transformed)
preds <- predict(knn.model, test.scaled)
preds[outliers.test.por.la.cara] <- 0
sub.47 <- preds
createSubmission(sub.47, "47") # 0.
```

```{r}
## SUBIDA 48
# EL orden inicial (imputacion -> filtro -> transformacion) pero sin tocar outliers
k <- 17
kernel <- "epanechnikov"
set.seed(28)
train.completed.3 <- knnImputation(train) 
train.cleaned.3 <- CVCF(train.completed.3, consensus = F)$cleanData
train.transformed.3 <- attr.transform.add(train.cleaned.3) %>% dplyr::select(-C, C) 
scaler <- preProcess(train.transformed.3)
train.scaled.3 <- predict(scaler, train.transformed.3)
knn.model <- train.kknn(C ~ ., train.scaled.3, ks = k, kernel = kernel, scale=F)

testt <- data.frame(test)
testt[outliers.test.por.la.cara,] <- 1 # Para que no estorben en las transformaciones
test.transformed <- attr.transform.add(testt)
test.scaled <- predict(scaler, test.transformed)
preds <- predict(knn.model, test.scaled)
preds[outliers.test.por.la.cara] <- 0
sub.48 <- preds
createSubmission(sub.48, "48") # 0.
```

```{r}
## SUBIDA 49
# Lo anterior + CNCA (1)
set.seed(28)
k <- 17
kernel <- "epanechnikov"
train.cnca.partition.1 <- function(i){
  train.cnca.i <- read.csv(paste0("./train_cnca2_x_",i,".csv"), header = F)
  train.cnca.y <- read.csv(paste0("./train_cnca2_y_",i,".csv"), header = F)
  train.cnca.i$C <- as.factor(train.cnca.y$V1)
  names(train.cnca.i) <- names(train.scaled.2)
  test.cnca.i <- read.csv(paste0("./test_cnca2_",i,".csv"), header = F)
  names(test.cnca.i) <- names(test.scaled)
  knn.model <- train.kknn(C ~ ., train.cnca.i, ks=k, kernel=kernel, scale=F)
  preds <- predict(knn.model, test.cnca.i)
  preds[outliers.test.por.la.cara] <- 0
  preds
}

sub.49.all <- sapply(0:8, train.cnca.partition.1)
sub.49 <- apply(sub.49.all, 1, function(x) ifelse(sum(x==1) > 4,1,0))
createSubmission(sub.49, "49") # 0.
```

```{r}
## SUBIDA 50
# Lo anterior + CNCA (1)
set.seed(28)
k <- 17
kernel <- "epanechnikov"
train.cnca.partition.2 <- function(i){
  train.cnca.i <- read.csv(paste0("./train_cnca2_",i,".csv"), header = F)
  train.cnca.i$C <- train.scaled.2$C
  names(train.cnca.i) <- names(train.scaled.2)
  test.cnca.i <- read.csv(paste0("./test_cnca2_",i,".csv"), header = F)
  names(test.cnca.i) <- names(test.scaled)
  knn.model <- train.kknn(C ~ ., train.cnca.i, ks=k, kernel=kernel, scale=F)
  preds <- predict(knn.model, test.cnca.i)
  preds[outliers.test.por.la.cara] <- 0
  preds
}

sub.50.all <- sapply(0:8, train.cnca.partition.2)
sub.50 <- apply(sub.50.all, 1, function(x) ifelse(sum(x==1) > 4,1,0))
createSubmission(sub.50, "50") # 0.
```



























```{r}
train.predict.j46 <- function(train, test){
  # Train
  train.cleaned.j <- CVCF(train, consensus = F)$cleanData
  outliers.train.por.la.cara.j <- which(apply(train.cleaned.j[,-ncol(train.cleaned.j)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara.j <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  # if(length(outliers.train.por.la.cara.j) > 0) train.cleaned.j <- train.cleaned.j[-outliers.train.por.la.cara.j,]
  
  
  train.transformed.j <- attr.transform.add(train.cleaned.j) %>% dplyr::select(-C,C)
  train.completed.j <- knnImputation(train.transformed.j) 
  # scaler <- preProcess(train.completed.j)
  # train.scaled.j <- predict(scaler, train.completed.j)
  
  model <- caret::train(C ~ ., train.completed.j, method="ctree", 
                        tuneGrid = expand.grid(mincriterion = c(0.95,0.94,0.93,0.92,0.91,0.90)))

  model$bestTune
  # completar con el que salga el mejor
  # model = J48(C~., data=train.scaled.j, control = Weka_control(M = , C = ))
  model = ctree(C ~ .,data = train.completed.j,control = ctree_control(mincriterion = 0.92))
  # Predict
  test[outliers.test.por.la.cara.j,] <- 1
  test.transformed.j <- attr.transform.add(test)
  # test.scaled.j <- predict(scaler, test.transformed.j)
  preds <- predict(model, test.transformed.j)
  preds[outliers.test.por.la.cara] <- 0
  return(preds)
}


train$C <- as.factor(train$C)
cross_validation(train, train.predict.knn.46)

set.seed(28)

sub.j46 <- train.predict.j46(train, test)
createSubmission(sub.j46, "j46")

```






```{r}

train.cleaned.j <- CVCF(train, consensus = F)$cleanData
  outliers.train.por.la.cara.j <- which(apply(train.cleaned.j[,-ncol(train.cleaned.j)], MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  outliers.test.por.la.cara.j <- which(apply(test, MARGIN=1, function(x) any(!is.na(x) & x < -68000)))
  if(length(outliers.train.por.la.cara.j) > 0) train.cleaned.j <- train.cleaned.j[-outliers.train.por.la.cara.j,]
  
  train.transformed.j <- attr.transform.add(train.cleaned.j) %>% dplyr::select(-C,C)
  train.completed.j <- knnImputation(train.transformed.j) 
  
  train.discretizado.j <- arulesCBA::discretizeDF.supervised(C~ .,train.completed.j, method= "mdlp")
  copia = train.discretizado.j
  model <- caret::train(C ~ ., train.discretizado.j, method="ctree",
                        tuneGrid = expand.grid(mincriterion = c(0.95,0.94,0.93,0.92,0.91,0.90)))
  model = ctree(C ~., data = train.discretizado.j, control = ctree_control(mincriterion = 0.95))

  testt <- data.frame(test)
  testt[outliers.test.por.la.cara.j,] <- 1
  test.transformed.j <- attr.transform.add(testt)
  test.discretized.j <- arules::discretizeDF(test.transformed.j,train.discretizado.j)
  preds <- predict(model, test.discretized.j)
  preds[outliers.test.por.la.cara.j] <- 0


```











